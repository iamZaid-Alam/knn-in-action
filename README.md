# ğŸŒ¸ KNN in Action â€” Iris Dataset Classification

<p align="center">
  <img src="https://img.shields.io/badge/Machine%20Learning-KNN-yellow?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Language-Python-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Framework-scikit--learn-brightgreen?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Status-Complete-success?style=for-the-badge"/>
</p>

<p align="center">
  <img src="iris.png" width="700" alt="Iris Dataset Visualization"/>
</p>

## ğŸ“– Overview

This project implements a comprehensive K-Nearest Neighbors (KNN) classifier on the famous Iris dataset, featuring advanced hyperparameter tuning, cross-validation strategies, and rich visualizations to understand model behavior and performance.

## âœ¨ Key Features

### ğŸ”§ Advanced Model Optimization
- **GridSearchCV** â€” Exhaustive search over specified parameter values
- **RandomizedSearchCV** â€” Efficient random sampling of hyperparameter space
- **Hyperparameter Tuning** â€” Optimizing K neighbors and distance metrics

### ğŸ“Š Comprehensive Evaluation
- **Nested Cross-Validation** â€” Unbiased performance estimation
- **Accuracy vs. K Analysis** â€” Understanding bias-variance tradeoff
- **Metric Comparison** â€” Euclidean, Manhattan, and Minkowski distances
- **Confusion Matrix** â€” Detailed classification breakdown

### ğŸ¨ Rich Visualizations

#### ğŸŒ€ Decision Boundaries (PCA 2D Projection)
Visualizes how KNN partitions the feature space, demonstrating class separability and decision boundary smoothness across different K values.

#### ğŸ›ï¸ Feature Importance (Permutation-based)
Reveals which features contribute most to classification accuracy. Petal dimensions (length and width) emerge as dominant predictors, aligning with botanical expectations.

#### ğŸ“ˆ Accuracy vs. K Curve
Illustrates the classic bias-variance tradeoff as K increases, helping identify the optimal number of neighbors.

#### ğŸ”¥ Performance Heatmap (K Ã— Distance Metric)
A comprehensive comparison matrix showing accuracy across:
- **Euclidean Distance** â€” Standard L2 norm
- **Manhattan Distance** â€” L1 norm for grid-based distance
- **Minkowski Distance** â€” Generalized metric

## ğŸ› ï¸ Technologies Used

- **Python 3.x**
- **scikit-learn** â€” ML algorithms and tools
- **NumPy** â€” Numerical computing
- **Pandas** â€” Data manipulation
- **Matplotlib/Seaborn** â€” Visualization
- **Colab Notebook** â€” Interactive development

---

## ğŸ“ˆ Results Summary

Both GridSearchCV and RandomizedSearchCV consistently achieve ~96â€“97% accuracy on the Iris dataset, with optimal performance occurring at K=7â€“10 using distance-weighted KNN and Euclidean/Minkowski metrics, while feature importance confirms that petal length and petal width are the dominant drivers of species classification.


## ğŸ™ Acknowledgments

- UCI Machine Learning Repository for the Iris dataset
- scikit-learn community for excellent documentation
- R.A. Fisher for the original Iris dataset (1936)

---

<p align="center">
  Made with â¤ï¸ for Machine Learning Education
</p>
